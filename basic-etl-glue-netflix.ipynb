{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 5.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nIdle Timeout: 2880\nSession ID: 060ca9c4-a6fd-4ee0-98f8-d7fd0b7055d0\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\nWaiting for session 060ca9c4-a6fd-4ee0-98f8-d7fd0b7055d0 to get into ready status...\nSession 060ca9c4-a6fd-4ee0-98f8-d7fd0b7055d0 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\nimport boto3 as boto\nimport json\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom datetime import datetime",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "client = boto.client('s3')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "file_contents = client.get_object(Bucket='aws-glue-sandy-demo', Key='config.json')['Body'].read().decode('utf-8')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "config = json.loads(file_contents)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "input_path = config['input_path']\noutput_path = config['output_path']\nprocessed_file_name = config['processed_file_name']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Extracting the S3 File 'netflix_dataset.csv' using GlueContext \ndyf = glueContext.create_dynamic_frame_from_options(connection_type=\"s3\", connection_options={\"paths\":[input_path]}, format=\"csv\", format_options={\"withHeader\":True})",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Converting DynamicFrame to PySpark DataFrame for further procesing of transformations\ndf = dyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Applying Schema casting for all columns to required datatypes\ndf = df.withColumn('index', col('index').cast(IntegerType()))\\\n.withColumn('id', col('id').cast(StringType()))\\\n.withColumn('title', col('title').cast(StringType()))\\\n.withColumn('type', col('type').cast(StringType()))\\\n.withColumn('description', col('description').cast(StringType()))\\\n.withColumn('release_year', col('release_year').cast(IntegerType()))\\\n.withColumn('age_certification', col('age_certification').cast(StringType()))\\\n.withColumn('runtime', col('runtime').cast(IntegerType()))\\\n.withColumn('imdb_id', col('imdb_id').cast(StringType()))\\\n.withColumn('imdb_score', col('imdb_score').cast(FloatType()))\\\n.withColumn('imdb_votes', col('imdb_votes').cast(IntegerType()))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Filtering only the MOVIE type and movie's release year to be greater than 2000\ndf = df.filter('type = \"MOVIE\" and release_year >= 2000')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Adding Processsed Timestamp and Processed File name columns\ndate_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\ndf = df.withColumn('processed_timestamp',lit(date_time)).withColumn('processed_filename',lit(processed_file_name))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Case when on Whether the Movie is children friendly or not - Binary Valued\ndf = df.select('*')\\\n    .withColumn('children_friendly',\\\n                when((col('age_certification') == 'PG') | (col('age_certification') == 'R')| (col('age_certification') == ''), True).otherwise(False))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Droppinng unwanted columns from the dataframe\ndf = df.drop('id','index','age_certification','type','description','imdb_id')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Adding Monotonically Unique Increasing ID value of numbers\ndf = df.withColumn('mono_id',monotonically_increasing_id())  ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Arranging the column to be in the below order before writing to S3\nselect_list = ['mono_id','title','release_year','children_friendly','runtime','imdb_score','imdb_votes','processed_timestamp','processed_filename']\ndf = df.select(select_list) ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Converting DataFrame to Dynamic Frame for writing to S3\ndyf = DynamicFrame.fromDF(df, glueContext)\n# glueContext.write_dynamic_frame_from_options(frame = dyf, connection_type= 's3', connection_options={'path':output_path}, format='parquet')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Writing Dynamic Frame to S3 and exposing the S3 Files to Athena as Table `netflix_movies` under Database `netflix_stats_db` via AWS Glue Data Catalog\ns3output = glueContext.getSink(\n  path=output_path,\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=['release_year'],\n  compression=\"snappy\",\n  enableUpdateCatalog=True\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"netflix_stats_db\", catalogTableName=\"netflix_movies\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fd33c709690>\n",
					"output_type": "stream"
				}
			]
		}
	]
}